# Computational Efficient Architectures for Natural Language Processing

This repository contains a paper (and eventually) a presentation about efficient architectures for NLP.
Following architectures are included:
- Traditional Architectures like RNN, CNN
- Transformer and some of its extensions (Performer, Roformer)
- Attention-Free Architectures
- Receptance Weighted Key Value (RWKV)
- State Space Models and its extensions (S4, Mamba, ...)

