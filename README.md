# Computational Efficient Architectures for Natural Language Processing

This repository contains a paper (and eventually) a presentation about efficient architectures for NLP.
Following architectures are included:
- Traditional Architectures like RNN, CNN
- Transformer and some of its extensions (Performer, Roformer)
- Attention-Free Architectures
- Receptance Weighted Key Value (RWKV)
- State Space Models and its extensions (S4, Mamba, ...)

This paper was created as part of the seminar "Computational Aspects of Machine Learning" (IN2183) during the WS24/25 
by myself and Roman Alyaev.

